{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b53b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'ear_thesh':0.33,\n",
    "    'mar_thesh' : 0.8,\n",
    "    'ear_frame_count_thesh' : 5,\n",
    "    'mar_frame_count_thesh' : 7,\n",
    "    'YOLOV5_phone_cigarette_model_weight': './weights/best_phone_cigaretteV3.pt',\n",
    "    'YOLOV5_conf': 0.6,\n",
    "    'phone_frame_count_thesh':3,\n",
    "    'cigarette_frame_count_thesh':3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ac5f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\SANG/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2023-1-6 Python-3.8.5 torch-1.13.0+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 157 layers, 1761871 parameters, 0 gradients, 4.1 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import time\n",
    "import threading\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_hands = mp.solutions.hands\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "YOLOV5_phone_cigarette_model = torch.hub.load('ultralytics/yolov5','custom', path = config['YOLOV5_phone_cigarette_model_weight'], force_reload=True)\n",
    "results_for_multithreading = {}\n",
    "\n",
    "def get_multile_face_landmarks(image):\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.7,\n",
    "        min_tracking_confidence=0.5) as face_mesh:\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        return results.multi_face_landmarks\n",
    "\n",
    "def get_multile_hand_landmarks(image):\n",
    "    with mp_hands.Hands(\n",
    "        static_image_mode=True,\n",
    "        max_num_hands=1,\n",
    "        min_detection_confidence=0.6) as hands:\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        return results\n",
    "\n",
    "def get_xyxy_phone_cigarette(frame, conf_thresh):\n",
    "    frame_in = frame.copy()\n",
    "    frame_in = cv2.cvtColor(frame_in, cv2.COLOR_BGR2RGB)\n",
    "    a = YOLOV5_phone_cigarette_model([frame_in])\n",
    "    labels, cords = a.xyxyn[0][:,-1], a.xyxyn[0][:,:-1]\n",
    "    y_shape, x_shape, _ = frame.shape\n",
    "    res = []\n",
    "    for info in a.xyxyn[0]:\n",
    "        x1, y1, x2, y2, conf, cls = int(info[0]*x_shape), int(info[1]*y_shape), int(info[2]*x_shape), int(info[3]*y_shape), info[4], int(info[5])\n",
    "        if conf>conf_thresh:\n",
    "            res.append((x1, y1, x2, y2, cls))\n",
    "    return res\n",
    "\n",
    "def draw_xyxy_phone_cigarette(frame, cords):\n",
    "    for cord in cords:\n",
    "        x1, y1, x2, y2, cls = cord\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0,0,255), 5)\n",
    "\n",
    "def draw_hand_landmark(image, hand_landmarks):\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        hand_landmarks,\n",
    "        mp_hands.HAND_CONNECTIONS,\n",
    "        mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "        mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "def draw_contours_frame(image, face_landmarks):\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=image,\n",
    "        landmark_list=face_landmarks,\n",
    "        connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_contours_style())\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=image,\n",
    "        landmark_list=face_landmarks,\n",
    "        connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp_drawing_styles\n",
    "        .get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "\n",
    "\n",
    "def get_xy_headpose(face_landmarks):\n",
    "    def preprocess_face_3d(face_3d):\n",
    "        b1 = (face_3d[1]+face_3d[2])/2\n",
    "        a1 = np.array([face_3d[0][0],b1[1],face_3d[0][2]])\n",
    "        c1 = np.array([b1[0],b1[1],face_3d[0][2]])\n",
    "        b2 = (face_3d[3]+face_3d[4])/2\n",
    "        a2 = np.array([b2[0], face_3d[0][1],face_3d[0][2]])\n",
    "        c2 = np.array([b2[0], b2[1],face_3d[0][2]])\n",
    "\n",
    "        return a1, b1, c1, a2, b2, c2\n",
    "    def cal_angle(ABC_coors):\n",
    "        a, b, c = ABC_coors[0], ABC_coors[1], ABC_coors[2]\n",
    "        ba = a - b\n",
    "        bc = c - b\n",
    "        cosine_angle = ba.dot(bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "        \n",
    "        angle = np.arccos(cosine_angle)\n",
    "        angle = np.degrees(angle)\n",
    "        if ba[0]<0 or ba[1]>0:\n",
    "            angle = -angle\n",
    "        return angle\n",
    "    arrayPoint = np.array([[point.x, point.y, point.z] for point in face_landmarks.landmark])\n",
    "    face_3d = arrayPoint[[1,129,358,4,2]].astype(np.float64)\n",
    "    a1, b1, c1, a2, b2, c2 = preprocess_face_3d(face_3d)\n",
    "    angle_y = cal_angle((a1, b1, c1))\n",
    "    angle_x = cal_angle((a2, b2, c2))\n",
    "    \n",
    "    return angle_x, angle_y, face_3d\n",
    "    \n",
    "def draw_xy_headpose(frame, angle_x, angle_y, face_3d):\n",
    "    img_h, img_w, img_c = frame.shape    \n",
    "    cv2.putText(frame,f\"XV2:{angle_x}\", (30, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "    cv2.putText(frame,f\"YV2:{angle_y}\", (30, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "\n",
    "    nose_2d = (int(face_3d[0][0]*img_w), int(face_3d[0][1]*img_h))\n",
    "\n",
    "    p2 = (int(nose_2d[0] + angle_y*10), int(nose_2d[1] - angle_x*10))\n",
    "    cv2.line(frame, nose_2d, p2, (255, 0, 0), 3)\n",
    "\n",
    "    \n",
    "\n",
    "def head_pose_handler(frame, face_landmarks):\n",
    "    img_h, img_w, img_c = frame.shape\n",
    "    arrayPoint = np.array([[point.x*img_w, point.y*img_h, point.z] for point in face_landmarks.landmark])\n",
    "    face_3d = arrayPoint[[1,33,61,199,263,291]].astype(np.float64)\n",
    "    face_2d = face_3d[:,:2].astype(np.float64)\n",
    "        \n",
    "    focal_length = 1*img_w\n",
    "    \n",
    "    cam_matrix = np.array([[focal_length, 0, img_h/2],\n",
    "                          [0, focal_length, img_w/2],\n",
    "                          [0, 0, 1]])\n",
    "    \n",
    "    dist_matrix = np.zeros((4,1))\n",
    "    \n",
    "    success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "    rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "    angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)\n",
    "    x, y, z = angles[0]*360, angles[1]*360, angles[2]*360 \n",
    "    \n",
    "    \n",
    "    \n",
    "    cv2.putText(frame,f\"X:{x}\", (30, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "    cv2.putText(frame,f\"Y:{y}\", (30, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "    \n",
    "    nose_2d = (int(arrayPoint[1][0]), int(arrayPoint[1][1]))\n",
    "    p2 = (int(nose_2d[0] + y*10), int(nose_2d[1] - x*10 +50))\n",
    "    cv2.line(frame, nose_2d, p2, (255, 0, 0), 3)\n",
    "    \n",
    "def return_variables(face_landmarks):\n",
    "    arrayPoint = np.array([[point.x, point.y, point.z] for point in face_landmarks.landmark])\n",
    "    def cal_aspect_ratio(four_points):\n",
    "        A = distance.euclidean(four_points[1], four_points[3])\n",
    "        C = distance.euclidean(four_points[0], four_points[2])\n",
    "        ear = (A) / (C)\n",
    "        return ear\n",
    "    def cal_eye_aspect_ratio(arrayPoint):\n",
    "        ear_1 = cal_aspect_ratio(arrayPoint[[33, 159, 133, 145]])\n",
    "        ear_2 = cal_aspect_ratio(arrayPoint[[362, 386, 263, 374]])\n",
    "        return (ear_1+ear_2)/2\n",
    "    def cal_mouth_aspect_ratio(arrayPoint):\n",
    "        return cal_aspect_ratio(arrayPoint[[78, 13, 308, 14]])\n",
    "    return cal_eye_aspect_ratio(arrayPoint), cal_mouth_aspect_ratio(arrayPoint)\n",
    "    \n",
    "class Alert_by_counting_frames():\n",
    "    def __init__(self):\n",
    "        self.ear_thesh = config['ear_thesh']\n",
    "        self.mar_thesh = config['mar_thesh']\n",
    "        self.ear_frame_count_thesh = config['ear_frame_count_thesh']\n",
    "        self.mar_frame_count_thesh = config['mar_frame_count_thesh']\n",
    "        self.phone_frame_count_thesh = config['phone_frame_count_thesh']\n",
    "        self.cigarette_frame_count_thesh = config['cigarette_frame_count_thesh']\n",
    "        self.number_of_func = 5\n",
    "\n",
    "        #do not change the variables below\n",
    "        self.dict_ar_lists = {\n",
    "            'ear':[0]*15,\n",
    "            'mar':[0]*15,\n",
    "            'phone':[0]*10,\n",
    "            'cigarette':[0]*10,\n",
    "        }\n",
    "\n",
    "    def put_warning_text(self, frame, text, idx):\n",
    "        cv2.putText(frame,text, (frame.shape[1]*idx//self.number_of_func, frame.shape[0]*9//10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 3)\n",
    "    \n",
    "    def recieve_values_ear_mar(self, frame, ear, mar):\n",
    "        if ear<self.ear_thesh:\n",
    "            self.dict_ar_lists['ear'].append(1)\n",
    "        else:\n",
    "            self.dict_ar_lists['ear'].append(0)\n",
    "            \n",
    "        if mar>self.mar_thesh:\n",
    "            self.dict_ar_lists['mar'].append(1)\n",
    "        else:\n",
    "            self.dict_ar_lists['mar'].append(0)\n",
    "        \n",
    "        self.dict_ar_lists['ear'].pop(0)\n",
    "        self.dict_ar_lists['mar'].pop(0)\n",
    "        \n",
    "        if np.sum(self.dict_ar_lists['ear'])>self.ear_frame_count_thesh:\n",
    "            self.put_warning_text(frame, \"SLEEPY\", 0)\n",
    "            # cv2.putText(frame,f\"SLEEPY\", (frame.shape[1]*0//self.number_of_func, frame.shape[0]*9//10), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 5)\n",
    "        \n",
    "        if np.sum(self.dict_ar_lists['mar'])>self.mar_frame_count_thesh:\n",
    "            self.put_warning_text(frame, \"YAWNING\", 1)\n",
    "            # cv2.putText(frame,f\"YAWNING\", (frame.shape[1]*1//self.number_of_func, frame.shape[0]*9//10), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 5)\n",
    "\n",
    "    \n",
    "    def recieve_values_hand(self, frame, multi_hands):\n",
    "        if multi_hands.multi_hand_landmarks != None:\n",
    "            self.put_warning_text(frame, \"HAND DETECTED\", 2)\n",
    "            # cv2.putText(frame,f\"HAND DETECTED\", (frame.shape[1]*2//self.number_of_func, frame.shape[0]*9//10), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 5)\n",
    "    \n",
    "    def recieve_values_phone_cigarette(self, frame, xyxy_phone_cigarette):\n",
    "        phone_detected, cigarette_detected = 0,0\n",
    "        for xyxy in xyxy_phone_cigarette:\n",
    "            cls = xyxy[-1]\n",
    "            if cls == 0:\n",
    "                phone_detected = 1\n",
    "            else:\n",
    "                cigarette_detected = 1\n",
    "       \n",
    "        self.dict_ar_lists['phone'].pop(0)\n",
    "        self.dict_ar_lists['cigarette'].pop(0)\n",
    "        self.dict_ar_lists['phone'].append(phone_detected)\n",
    "        self.dict_ar_lists['cigarette'].append(cigarette_detected)\n",
    "        if np.sum(self.dict_ar_lists['phone'])>self.phone_frame_count_thesh:\n",
    "            self.put_warning_text(frame, \"PHONE DETECTED\", 3)\n",
    "        if np.sum(self.dict_ar_lists['cigarette'])>self.cigarette_frame_count_thesh:\n",
    "            self.put_warning_text(frame, \"CIGARETTE DETECTED\", 4)\n",
    "        \n",
    "\n",
    "class CustomThread(threading.Thread):\n",
    "    def __init__(self, group=None, target=None, name=None,\n",
    "                 args=(), kwargs={}, Verbose=None):\n",
    "        threading.Thread.__init__(self, group, target, name, args, kwargs)\n",
    "        self._return = None\n",
    " \n",
    "    def run(self):\n",
    "        if self._target is not None:\n",
    "            self._return = self._target(*self._args, **self._kwargs)\n",
    "             \n",
    "    def join(self, *args):\n",
    "        threading.Thread.join(self, *args)\n",
    "        return self._return    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "426cf767",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "writer = cv2.VideoWriter('out5.avi', fourcc, 5, (640, 480*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f80083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "\n",
    "alert = Alert_by_counting_frames()\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        print(\"Ignoring empty camera frame.\")\n",
    "        # If loading a video, use 'break' instead of 'continue'.\n",
    "        continue\n",
    "    start = time.time()    \n",
    "    image_org = cv2.flip(image.copy(), 1)\n",
    "    image = cv2.flip(image, 1)\n",
    "    \n",
    "    #define process \n",
    "    process_1 = CustomThread(target=get_multile_face_landmarks, args=(image,))\n",
    "    process_2 = CustomThread(target=get_multile_hand_landmarks, args=(image,))\n",
    "    process_3 = CustomThread(target=get_xyxy_phone_cigarette, args=(image, config['YOLOV5_conf'],))\n",
    "\n",
    "    process_1.start()\n",
    "    process_2.start()\n",
    "    process_3.start()\n",
    "\n",
    "    multi_face_landmarks = process_1.join()\n",
    "    multi_hands = process_2.join()\n",
    "    xyxy_phone_cigarette = process_3.join()\n",
    "\n",
    "    # #get infromation from fram\n",
    "    # multi_face_landmarks = get_multile_face_landmarks(image)\n",
    "    # multi_hands = get_multile_hand_landmarks(image)\n",
    "    # xyxy_phone_cigarette = get_xyxy_phone_cigarette(image, config['YOLOV5_conf'])\n",
    "\n",
    "    alert.recieve_values_phone_cigarette(image, xyxy_phone_cigarette)\n",
    "\n",
    "    if multi_face_landmarks!= None:\n",
    "        for face_landmarks in multi_face_landmarks:\n",
    "            #get the headpose of face\n",
    "            angle_x, angle_y, face_3d = get_xy_headpose(face_landmarks)\n",
    "            \n",
    "            #draw head pose and contour\n",
    "            draw_xy_headpose(image, angle_x, angle_y, face_3d)\n",
    "            draw_contours_frame(image, face_landmarks)\n",
    "            draw_xyxy_phone_cigarette(image, xyxy_phone_cigarette)\n",
    "            \n",
    "            ear, mar = return_variables(face_landmarks)\n",
    "            alert.recieve_values_ear_mar(image, ear, mar)\n",
    "            \n",
    "            #draw EAR, MAR\n",
    "            cv2.putText(image,f\"EAR: {ear}\", (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "            cv2.putText(image,f\"MAR: {mar}\", (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "\n",
    "    if multi_hands.multi_hand_landmarks!= None:\n",
    "        for hand_landmarks in multi_hands.multi_hand_landmarks:\n",
    "            draw_hand_landmark(image, hand_landmarks)\n",
    "            alert.recieve_values_hand(image, multi_hands)\n",
    "\n",
    "    cv2.putText(image,f\"FPS: {round(1/(time.time()-start),2)}\", (450, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)                \n",
    "    cv2.imshow('MediaPipe Face Mesh', np.concatenate([image, image_org]))\n",
    "    writer.write( np.concatenate([image, image_org]))\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "writer.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (tags/v3.8.5:580fbb0, Jul 20 2020, 15:57:54) [MSC v.1924 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "0915971c03707a21584c0aabab848a3b147512c180720cb6e7127d50e0b92195"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
